{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64710118-69f6-4720-955f-97b1b461a815",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in directory: station_data\n",
      "Total CSV files found: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_stations_from_networks():\n",
    "    \"\"\"Build a station list by using IEM networks.\"\"\"\n",
    "    stations = []\n",
    "    states = (\n",
    "        \"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME MI MN \"\n",
    "        \"MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT \"\n",
    "        \"WA WI WV WY\"\n",
    "    )\n",
    "    networks = [f\"{state}_ASOS\" for state in states.split()]\n",
    "\n",
    "    for network in networks:\n",
    "        # Get metadata\n",
    "        uri = f\"https://mesonet.agron.iastate.edu/geojson/network/{network}.geojson\"\n",
    "        try:\n",
    "            response = requests.get(uri)\n",
    "            response.raise_for_status()  # Check for request errors\n",
    "            jdict = response.json()\n",
    "            \n",
    "            # Extract station IDs\n",
    "            for site in jdict[\"features\"]:\n",
    "                stations.append(site[\"properties\"][\"sid\"])\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to retrieve data for network {network}: {e}\")\n",
    "    \n",
    "    return stations\n",
    "\n",
    "def download_station_data(station_list, output_dir, base_url=None):\n",
    "    \"\"\"\n",
    "    Download data for each station from the specified or default base URL and save as CSV files.\n",
    "    \"\"\"\n",
    "    # Set a default base URL if none is provided\n",
    "    if base_url is None:\n",
    "        base_url = (\n",
    "            \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "            \"station=STATION_ID&data=feel&year1=1993&month1=1&day1=1&year2=2023&month2=1&day2=1&\"\n",
    "            \"tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T&direct=no&report_type=3\"\n",
    "        )\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for station in station_list:\n",
    "        if station is None:  # Skip invalid entries\n",
    "            continue\n",
    "\n",
    "        # Construct the URL for each station\n",
    "        url = base_url.replace(\"STATION_ID\", station)\n",
    "        \n",
    "        # Download and save the file\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Check for request errors\n",
    "            \n",
    "            # Define the output file path\n",
    "            output_file = os.path.join(output_dir, f\"{station}_data.csv\")\n",
    "            \n",
    "            # Write to CSV file\n",
    "            with open(output_file, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "                \n",
    "            print(f\"Downloaded data for station: {station}\")\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to download data for station: {station}. Error: {e}\")    \n",
    "\n",
    "def get_cleaned_station_id(station_id):\n",
    "    \"\"\"Apply cleaning to a station ID and return the cleaned version.\"\"\"\n",
    "    if station_id[0].isdigit():  # Exclude IDs that start with a number\n",
    "        return None  # Return None or raise an exception if desired\n",
    "    elif len(station_id) == 4:  # Keep IDs that are exactly 4 characters long\n",
    "        cleaned_station = station_id\n",
    "    else:  # Add \"K\" in front of IDs shorter than 4 characters\n",
    "        cleaned_station = f\"K{station_id}\"\n",
    "    return cleaned_station\n",
    "\n",
    "def get_file_paths(directory):\n",
    "    \"\"\"\n",
    "    Get all file paths in a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory: The path to the directory to search in.\n",
    "\n",
    "    Returns:\n",
    "    - A list of full file paths for all files in the directory.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    print(f\"Searching in directory: {directory}\")  # Debug statement\n",
    "    for root, _, files in os.walk(directory):\n",
    "        print(f\"Current directory: {root}\")  # Debug statement\n",
    "        for file in files:\n",
    "            print(f\"Found file: {file}\")  # Debug statement\n",
    "            if file.lower().endswith(\".csv\"):\n",
    "                dataset.append(os.path.join(root, file))\n",
    "    print(f\"Total CSV files found: {len(dataset)}\")  # Debug statement\n",
    "    return dataset\n",
    "\n",
    "# Function to handle ICAO call sign extraction and data loading\n",
    "def handle_station_data(file_path):\n",
    "    \"\"\"Read CSV file and extract ICAO call sign or return a default value.\"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Check if the DataFrame is empty\n",
    "    if df.empty:\n",
    "        print(f\"Warning: The file {file_path} is empty. Returning a default ICAO code.\")\n",
    "        return \"KUNKNOWN\", pd.DataFrame()  # Return a default or placeholder value\n",
    "\n",
    "    icao_code = f\"K{df.iloc[0, 0]}\"\n",
    "    return icao_code, df  # Return both ICAO code and the DataFrame\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Load specific columns from a CSV file and preprocess data types.\"\"\"\n",
    "    data = pd.read_csv(file_path, usecols=[1, 2])  # Assuming columns 1 and 2 are date and temp\n",
    "    data[\"valid\"] = pd.to_datetime(data[\"valid\"], errors=\"coerce\")\n",
    "    data[\"feel\"] = pd.to_numeric(data[\"feel\"], errors=\"coerce\")\n",
    "    return data\n",
    "\n",
    "def convert_to_timezone(data, time_zone):\n",
    "    \"\"\"Convert datetime column to specified timezone and remove timezone information.\"\"\"\n",
    "    data[\"valid\"] = data[\"valid\"].dt.tz_localize(\"UTC\").dt.tz_convert(time_zone).dt.tz_localize(None)\n",
    "    return data\n",
    "\n",
    "def filter_daylight_hours(data):\n",
    "    \"\"\"Filter data for times between 6 am and 6 pm.\"\"\"\n",
    "    return data[(data[\"valid\"].dt.hour >= 6) & (data[\"valid\"].dt.hour < 18)]\n",
    "\n",
    "def filter_ideal_temps(data):\n",
    "    \"\"\"Filter data for temperatures between 50 and 63.5 degrees.\"\"\"\n",
    "    return data[(data[\"feel\"] >= 50) & (data[\"feel\"] <= 63.5)]\n",
    "\n",
    "def get_complete_years(data):\n",
    "    \"\"\"Identify years with data spanning from January to December.\"\"\"\n",
    "    complete_years = []\n",
    "    for year in data[\"valid\"].dt.year.unique():\n",
    "        year_data = data[data[\"valid\"].dt.year == year]\n",
    "        if (year_data[\"valid\"].dt.month.min() == 1) and (year_data[\"valid\"].dt.month.max() == 12):\n",
    "            complete_years.append(year)\n",
    "    return complete_years\n",
    "\n",
    "def filter_complete_years(data, complete_years):\n",
    "    \"\"\"Filter data to include only entries from complete years.\"\"\"\n",
    "    return data[data[\"valid\"].dt.year.isin(complete_years)]\n",
    "\n",
    "def calculate_average_ideal_days_per_year(filtered_data, complete_years):\n",
    "    \"\"\"Calculate the average number of ideal temperature days per year.\"\"\"\n",
    "    count_days = filtered_data[\"valid\"].dt.date.nunique()\n",
    "    num_years = len(complete_years)\n",
    "    return count_days / num_years if num_years > 0 else 0\n",
    "\n",
    "# Main function to execute the full pipeline\n",
    "def average_annual_ideal_run_temp_days(file_path, time_zone):\n",
    "    \"\"\"Calculate the average annual ideal run temperature days for the dataset.\"\"\"\n",
    "    icao_code, data = handle_station_data(file_path)\n",
    "    if data.empty:\n",
    "        return 0  # Skip processing if the DataFrame is empty\n",
    "\n",
    "    data = load_and_preprocess_data(file_path)\n",
    "    data = convert_to_timezone(data, time_zone)\n",
    "    data_daylight = filter_daylight_hours(data)\n",
    "    filtered_data = filter_ideal_temps(data_daylight)\n",
    "    complete_years = get_complete_years(filtered_data)\n",
    "    complete_data = filter_complete_years(filtered_data, complete_years)\n",
    "    return calculate_average_ideal_days_per_year(complete_data, complete_years)\n",
    "\n",
    "def write_all_results_to_csv(dataset, output_path=None):\n",
    "    \"\"\"Iterate over the dataset and write ICAO codes and average annual ideal run temp days to a CSV file.\"\"\"\n",
    "    # Set default output path with date and time if none is provided\n",
    "    if output_path is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        output_path = f\"results_{timestamp}.csv\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for station_id, time_zone in dataset.items():\n",
    "        cleaned_station = get_cleaned_station_id(station_id)  # Get cleaned station ID\n",
    "        if cleaned_station:  # Check if the cleaned station ID is valid\n",
    "            aairtd = average_annual_ideal_run_temp_days(station_id, time_zone)\n",
    "            \n",
    "            # Append the result as a dictionary\n",
    "            results.append({\"ICAO\": cleaned_station, \"aairtd\": aairtd})\n",
    "\n",
    "    # Convert results to a DataFrame and write to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results written to {output_path}\")\n",
    "\n",
    "print(get_file_paths(\"station_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643d910-97bd-4409-b55a-cf548605031f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
